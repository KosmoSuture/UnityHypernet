{
  "address": "0.1.15",
  "type_address": null,
  "data": {
    "name": "import_structure.py",
    "type": "file",
    "extension": ".py",
    "path": "C:\\Hypernet\\Hypernet Structure\\0\\0.1 - Hypernet Core\\import_structure.py",
    "size": 10796,
    "content": "\"\"\"\nImport Existing Hypernet Structure\n\nWalks the Hypernet Structure folder tree and imports it as nodes and links\ninto the Hypernet graph store. This makes the existing documentation,\nidentity files, and planning documents queryable through the API.\n\nMapping logic:\n  Folder \"0/0.1 - Hypernet Core\"  ->  Node at address \"0.0.1\" (approx)\n  File \"README.md\" inside a folder ->  Content stored in the node's data\n\nFor the initial import, we use a simplified addressing scheme that maps\nfolder names to addresses where possible, and generates sequential\naddresses where the folder name doesn't contain an address.\n\nUsage:\n  python import_structure.py [--data-dir data] [--source-dir .]\n\"\"\"\n\nimport sys\nimport re\nimport json\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom hypernet.address import HypernetAddress\nfrom hypernet.node import Node\nfrom hypernet.link import Link\nfrom hypernet.store import Store\n\n\n# Maps top-level folder names to Hypernet address categories\nCATEGORY_MAP = {\n    \"0\": \"0\",\n    \"1 - People\": \"1\",\n    \"2 - AI Accounts\": \"2\",\n    \"2 - AI Entities\": \"2\",\n    \"3 - Businesses\": \"3\",\n    \"4 - Knowledge\": \"4\",\n    \"4 -  Knowledge\": \"4\",\n    \"5 - Objects\": \"5\",\n    \"5 - People of History\": \"5\",\n    \"6 - People of History\": \"6\",\n    \"9 - Aliases\": \"9\",\n}\n\n# Pattern to extract address-like numbers from folder names\nADDRESS_PATTERN = re.compile(r'^(\\d+(?:\\.\\d+)*)')\nFOLDER_NUM_PATTERN = re.compile(r'^(\\d+(?:\\.\\d+)*)\\s*-?\\s*(.*)')\n\n\ndef extract_address_from_name(name: str) -> tuple[str | None, str]:\n    \"\"\"\n    Try to extract a Hypernet address prefix from a folder/file name.\n    Returns (address_prefix_or_None, clean_name).\n\n    Examples:\n      \"0.1 - Hypernet Core\" -> (\"0.1\", \"Hypernet Core\")\n      \"2.1 - Claude Opus (First AI Citizen)\" -> (\"2.1\", \"Claude Opus (First AI Citizen)\")\n      \"README.md\" -> (None, \"README.md\")\n    \"\"\"\n    match = FOLDER_NUM_PATTERN.match(name)\n    if match:\n        return match.group(1), match.group(2).strip()\n    return None, name\n\n\ndef import_folder(\n    store: Store,\n    folder_path: Path,\n    address_prefix: str,\n    parent_address: str | None = None,\n    depth: int = 0,\n    max_depth: int = 8,\n) -> int:\n    \"\"\"\n    Recursively import a folder and its contents as nodes.\n    Returns count of nodes created.\n    \"\"\"\n    if depth > max_depth:\n        return 0\n\n    count = 0\n\n    # Create node for this folder\n    folder_data = {\n        \"name\": folder_path.name,\n        \"type\": \"folder\",\n        \"path\": str(folder_path),\n    }\n\n    # Check for README.md or similar content files\n    for content_file in [\"README.md\", \"node.json\", \"index.md\"]:\n        content_path = folder_path / content_file\n        if content_path.exists():\n            try:\n                text = content_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n                # Truncate very long files for the data field\n                if len(text) > 10000:\n                    folder_data[\"content_preview\"] = text[:10000] + \"\\n... [truncated]\"\n                else:\n                    folder_data[\"content\"] = text\n                folder_data[\"content_file\"] = content_file\n            except Exception:\n                pass\n\n    try:\n        ha = HypernetAddress.parse(address_prefix)\n        node = Node(\n            address=ha,\n            data=folder_data,\n            source_type=\"import\",\n            source_id=f\"folder:{folder_path.name}\",\n        )\n        store.put_node(node)\n        count += 1\n\n        # Link to parent\n        if parent_address:\n            try:\n                link = Link(\n                    from_address=HypernetAddress.parse(parent_address),\n                    to_address=ha,\n                    link_type=\"0.6.3\",\n                    relationship=\"contains\",\n                )\n                store.put_link(link)\n            except ValueError:\n                pass  # Skip invalid links\n    except ValueError as e:\n        print(f\"  Warning: Skipping invalid address '{address_prefix}': {e}\")\n        return 0\n\n    # Process subfolders\n    # First pass: collect all explicitly-addressed children to avoid collisions\n    if folder_path.is_dir():\n        reserved_suffixes: set[str] = set()\n        children = sorted(\n            [c for c in folder_path.iterdir()\n             if not c.name.startswith(\".\") and c.name != \"__pycache__\"],\n            key=lambda c: c.name,\n        )\n\n        for child in children:\n            if child.is_dir():\n                addr_part, _ = extract_address_from_name(child.name)\n                if addr_part:\n                    # Track the last segment to avoid collision with sequential numbering\n                    if addr_part.startswith(address_prefix + \".\"):\n                        suffix = addr_part[len(address_prefix) + 1:]\n                    elif addr_part.startswith(address_prefix):\n                        suffix = addr_part[len(address_prefix):]\n                    else:\n                        suffix = addr_part\n                    # Track the first segment of the suffix (e.g., \"1\" from \"1.2.3\")\n                    first_seg = suffix.split(\".\")[0] if suffix else \"\"\n                    if first_seg.isdigit():\n                        reserved_suffixes.add(int(first_seg))\n\n        # Sequential counter starts above the highest reserved number\n        child_counter = max(reserved_suffixes) if reserved_suffixes else 0\n\n        for child in children:\n            if child.is_dir():\n                addr_part, clean_name = extract_address_from_name(child.name)\n\n                if addr_part:\n                    child_address = addr_part\n                    # If the extracted address doesn't start with our prefix,\n                    # make it relative\n                    if not child_address.startswith(address_prefix):\n                        child_address = f\"{address_prefix}.{addr_part}\"\n                else:\n                    child_counter += 1\n                    # Skip any numbers that collide with named folders\n                    while child_counter in reserved_suffixes:\n                        child_counter += 1\n                    child_address = f\"{address_prefix}.{child_counter}\"\n\n                count += import_folder(\n                    store, child, child_address,\n                    parent_address=address_prefix,\n                    depth=depth + 1,\n                    max_depth=max_depth,\n                )\n\n            elif child.is_file() and child.suffix in (\".md\", \".txt\", \".py\", \".sql\", \".json\"):\n                # Import individual files as leaf nodes\n                child_counter += 1\n                # Skip any numbers that collide with named folders\n                while child_counter in reserved_suffixes:\n                    child_counter += 1\n                file_address = f\"{address_prefix}.{child_counter}\"\n\n                file_data = {\n                    \"name\": child.name,\n                    \"type\": \"file\",\n                    \"extension\": child.suffix,\n                    \"path\": str(child),\n                    \"size\": child.stat().st_size,\n                }\n\n                # Read small files\n                if child.stat().st_size < 50000:\n                    try:\n                        file_data[\"content\"] = child.read_text(encoding=\"utf-8\", errors=\"replace\")\n                    except Exception:\n                        pass\n\n                try:\n                    file_ha = HypernetAddress.parse(file_address)\n                    file_node = Node(\n                        address=file_ha,\n                        data=file_data,\n                        source_type=\"import\",\n                        source_id=f\"file:{child.name}\",\n                    )\n                    store.put_node(file_node)\n                    count += 1\n\n                    # Link file to parent folder\n                    link = Link(\n                        from_address=ha,\n                        to_address=file_ha,\n                        link_type=\"0.6.3\",\n                        relationship=\"contains\",\n                    )\n                    store.put_link(link)\n                except ValueError:\n                    pass\n\n    return count\n\n\ndef import_structure(source_dir: str = \".\", data_dir: str = \"data\") -> dict:\n    \"\"\"\n    Import the entire Hypernet Structure into the graph store.\n    Returns import statistics.\n    \"\"\"\n    source = Path(source_dir).resolve()\n    store = Store(data_dir)\n\n    # Defer index saves during import (saves ~3600 file writes).\n    # Without this, Windows I/O can fail from rapid successive writes to the\n    # same index files. Indexes are saved once at the end.\n    _real_save_indexes = store._save_indexes\n    store._save_indexes = lambda: None\n\n    print(f\"Importing from: {source}\")\n    print(f\"Storing to: {Path(data_dir).resolve()}\")\n    print()\n\n    total_nodes = 0\n\n    # Process top-level folders\n    for child in sorted(source.iterdir()):\n        if child.name.startswith(\".\") or not child.is_dir():\n            continue\n\n        # Map folder name to category\n        category = CATEGORY_MAP.get(child.name)\n\n        if category is None:\n            # Try extracting address from name\n            addr_part, clean_name = extract_address_from_name(child.name)\n            if addr_part:\n                category = addr_part\n            else:\n                continue  # Skip unmapped folders\n\n        print(f\"[{category}.*] Importing: {child.name}\")\n        count = import_folder(store, child, category, depth=0)\n        print(f\"       -> {count} nodes created\")\n        total_nodes += count\n\n    # Save indexes once at the end\n    _real_save_indexes()\n    print(\"  Indexes saved.\")\n\n    stats = store.stats()\n    print(f\"\\n=== Import Complete ===\")\n    print(f\"Total nodes: {stats['total_nodes']}\")\n    print(f\"Total links: {stats['total_links']}\")\n    print(f\"Types: {stats['types']}\")\n    print(f\"Owners: {stats['owners']}\")\n\n    return stats\n\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Import Hypernet Structure into graph store\")\n    parser.add_argument(\"--source-dir\", default=\"C:/Hypernet/Hypernet Structure\",\n                       help=\"Path to Hypernet Structure folder\")\n    parser.add_argument(\"--data-dir\", default=\"C:/Hypernet/Hypernet Structure/0/0.1 - Hypernet Core/data\",\n                       help=\"Path to store graph data\")\n    args = parser.parse_args()\n\n    import_structure(args.source_dir, args.data_dir)\n"
  },
  "created_at": "2026-02-17T01:23:28.575951+00:00",
  "updated_at": "2026-02-17T01:23:28.575951+00:00",
  "deleted_at": null,
  "source_type": "import",
  "source_id": "file:import_structure.py"
}