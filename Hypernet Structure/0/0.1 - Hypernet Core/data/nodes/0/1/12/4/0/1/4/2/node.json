{
  "address": "0.1.12.4.0.1.4.2",
  "type_address": null,
  "data": {
    "name": "node.json",
    "type": "file",
    "extension": ".json",
    "path": "C:\\Hypernet\\Hypernet Structure\\0\\0.1 - Hypernet Core\\data-backup-pre-collision-fix\\nodes\\0\\1\\4\\node.json",
    "size": 9582,
    "content": "{\n  \"address\": \"0.1.4\",\n  \"type_address\": null,\n  \"data\": {\n    \"name\": \"import_structure.py\",\n    \"type\": \"file\",\n    \"extension\": \".py\",\n    \"path\": \"C:\\\\Hypernet\\\\Hypernet Structure\\\\0\\\\0.1 - Hypernet Core\\\\import_structure.py\",\n    \"size\": 8875,\n    \"content\": \"\\\"\\\"\\\"\\nImport Existing Hypernet Structure\\n\\nWalks the Hypernet Structure folder tree and imports it as nodes and links\\ninto the Hypernet graph store. This makes the existing documentation,\\nidentity files, and planning documents queryable through the API.\\n\\nMapping logic:\\n  Folder \\\"0/0.1 - Hypernet Core\\\"  ->  Node at address \\\"0.0.1\\\" (approx)\\n  File \\\"README.md\\\" inside a folder ->  Content stored in the node's data\\n\\nFor the initial import, we use a simplified addressing scheme that maps\\nfolder names to addresses where possible, and generates sequential\\naddresses where the folder name doesn't contain an address.\\n\\nUsage:\\n  python import_structure.py [--data-dir data] [--source-dir .]\\n\\\"\\\"\\\"\\n\\nimport sys\\nimport re\\nimport json\\nfrom pathlib import Path\\nfrom datetime import datetime, timezone\\n\\nsys.path.insert(0, str(Path(__file__).parent))\\n\\nfrom hypernet.address import HypernetAddress\\nfrom hypernet.node import Node\\nfrom hypernet.link import Link\\nfrom hypernet.store import Store\\n\\n\\n# Maps top-level folder names to Hypernet address categories\\nCATEGORY_MAP = {\\n    \\\"0\\\": \\\"0\\\",\\n    \\\"1 - People\\\": \\\"1\\\",\\n    \\\"2 - AI Accounts\\\": \\\"2\\\",\\n    \\\"2 - AI Entities\\\": \\\"2\\\",\\n    \\\"3 - Businesses\\\": \\\"3\\\",\\n    \\\"4 - Knowledge\\\": \\\"4\\\",\\n    \\\"4 -  Knowledge\\\": \\\"4\\\",\\n    \\\"5 - Objects\\\": \\\"5\\\",\\n    \\\"5 - People of History\\\": \\\"5\\\",\\n    \\\"6 - People of History\\\": \\\"6\\\",\\n    \\\"9 - Aliases\\\": \\\"9\\\",\\n}\\n\\n# Pattern to extract address-like numbers from folder names\\nADDRESS_PATTERN = re.compile(r'^(\\\\d+(?:\\\\.\\\\d+)*)')\\nFOLDER_NUM_PATTERN = re.compile(r'^(\\\\d+(?:\\\\.\\\\d+)*)\\\\s*-?\\\\s*(.*)')\\n\\n\\ndef extract_address_from_name(name: str) -> tuple[str | None, str]:\\n    \\\"\\\"\\\"\\n    Try to extract a Hypernet address prefix from a folder/file name.\\n    Returns (address_prefix_or_None, clean_name).\\n\\n    Examples:\\n      \\\"0.1 - Hypernet Core\\\" -> (\\\"0.1\\\", \\\"Hypernet Core\\\")\\n      \\\"2.1 - Claude Opus (First AI Citizen)\\\" -> (\\\"2.1\\\", \\\"Claude Opus (First AI Citizen)\\\")\\n      \\\"README.md\\\" -> (None, \\\"README.md\\\")\\n    \\\"\\\"\\\"\\n    match = FOLDER_NUM_PATTERN.match(name)\\n    if match:\\n        return match.group(1), match.group(2).strip()\\n    return None, name\\n\\n\\ndef import_folder(\\n    store: Store,\\n    folder_path: Path,\\n    address_prefix: str,\\n    parent_address: str | None = None,\\n    depth: int = 0,\\n    max_depth: int = 8,\\n) -> int:\\n    \\\"\\\"\\\"\\n    Recursively import a folder and its contents as nodes.\\n    Returns count of nodes created.\\n    \\\"\\\"\\\"\\n    if depth > max_depth:\\n        return 0\\n\\n    count = 0\\n\\n    # Create node for this folder\\n    folder_data = {\\n        \\\"name\\\": folder_path.name,\\n        \\\"type\\\": \\\"folder\\\",\\n        \\\"path\\\": str(folder_path),\\n    }\\n\\n    # Check for README.md or similar content files\\n    for content_file in [\\\"README.md\\\", \\\"node.json\\\", \\\"index.md\\\"]:\\n        content_path = folder_path / content_file\\n        if content_path.exists():\\n            try:\\n                text = content_path.read_text(encoding=\\\"utf-8\\\", errors=\\\"replace\\\")\\n                # Truncate very long files for the data field\\n                if len(text) > 10000:\\n                    folder_data[\\\"content_preview\\\"] = text[:10000] + \\\"\\\\n... [truncated]\\\"\\n                else:\\n                    folder_data[\\\"content\\\"] = text\\n                folder_data[\\\"content_file\\\"] = content_file\\n            except Exception:\\n                pass\\n\\n    try:\\n        ha = HypernetAddress.parse(address_prefix)\\n        node = Node(\\n            address=ha,\\n            data=folder_data,\\n            source_type=\\\"import\\\",\\n            source_id=f\\\"folder:{folder_path.name}\\\",\\n        )\\n        store.put_node(node)\\n        count += 1\\n\\n        # Link to parent\\n        if parent_address:\\n            try:\\n                link = Link(\\n                    from_address=HypernetAddress.parse(parent_address),\\n                    to_address=ha,\\n                    link_type=\\\"0.6.3\\\",\\n                    relationship=\\\"contains\\\",\\n                )\\n                store.put_link(link)\\n            except ValueError:\\n                pass  # Skip invalid links\\n    except ValueError as e:\\n        print(f\\\"  Warning: Skipping invalid address '{address_prefix}': {e}\\\")\\n        return 0\\n\\n    # Process subfolders\\n    child_counter = 0\\n    if folder_path.is_dir():\\n        for child in sorted(folder_path.iterdir()):\\n            if child.name.startswith(\\\".\\\") or child.name == \\\"__pycache__\\\":\\n                continue\\n\\n            if child.is_dir():\\n                addr_part, clean_name = extract_address_from_name(child.name)\\n\\n                if addr_part:\\n                    child_address = addr_part\\n                    # If the extracted address doesn't start with our prefix,\\n                    # make it relative\\n                    if not child_address.startswith(address_prefix):\\n                        child_address = f\\\"{address_prefix}.{addr_part}\\\"\\n                else:\\n                    child_counter += 1\\n                    child_address = f\\\"{address_prefix}.{child_counter}\\\"\\n\\n                count += import_folder(\\n                    store, child, child_address,\\n                    parent_address=address_prefix,\\n                    depth=depth + 1,\\n                    max_depth=max_depth,\\n                )\\n\\n            elif child.is_file() and child.suffix in (\\\".md\\\", \\\".txt\\\", \\\".py\\\", \\\".sql\\\", \\\".json\\\"):\\n                # Import individual files as leaf nodes\\n                child_counter += 1\\n                file_address = f\\\"{address_prefix}.{child_counter}\\\"\\n\\n                file_data = {\\n                    \\\"name\\\": child.name,\\n                    \\\"type\\\": \\\"file\\\",\\n                    \\\"extension\\\": child.suffix,\\n                    \\\"path\\\": str(child),\\n                    \\\"size\\\": child.stat().st_size,\\n                }\\n\\n                # Read small files\\n                if child.stat().st_size < 50000:\\n                    try:\\n                        file_data[\\\"content\\\"] = child.read_text(encoding=\\\"utf-8\\\", errors=\\\"replace\\\")\\n                    except Exception:\\n                        pass\\n\\n                try:\\n                    file_ha = HypernetAddress.parse(file_address)\\n                    file_node = Node(\\n                        address=file_ha,\\n                        data=file_data,\\n                        source_type=\\\"import\\\",\\n                        source_id=f\\\"file:{child.name}\\\",\\n                    )\\n                    store.put_node(file_node)\\n                    count += 1\\n\\n                    # Link file to parent folder\\n                    link = Link(\\n                        from_address=ha,\\n                        to_address=file_ha,\\n                        link_type=\\\"0.6.3\\\",\\n                        relationship=\\\"contains\\\",\\n                    )\\n                    store.put_link(link)\\n                except ValueError:\\n                    pass\\n\\n    return count\\n\\n\\ndef import_structure(source_dir: str = \\\".\\\", data_dir: str = \\\"data\\\") -> dict:\\n    \\\"\\\"\\\"\\n    Import the entire Hypernet Structure into the graph store.\\n    Returns import statistics.\\n    \\\"\\\"\\\"\\n    source = Path(source_dir).resolve()\\n    store = Store(data_dir)\\n\\n    print(f\\\"Importing from: {source}\\\")\\n    print(f\\\"Storing to: {Path(data_dir).resolve()}\\\")\\n    print()\\n\\n    total_nodes = 0\\n\\n    # Process top-level folders\\n    for child in sorted(source.iterdir()):\\n        if child.name.startswith(\\\".\\\") or not child.is_dir():\\n            continue\\n\\n        # Map folder name to category\\n        category = CATEGORY_MAP.get(child.name)\\n\\n        if category is None:\\n            # Try extracting address from name\\n            addr_part, clean_name = extract_address_from_name(child.name)\\n            if addr_part:\\n                category = addr_part\\n            else:\\n                continue  # Skip unmapped folders\\n\\n        print(f\\\"[{category}.*] Importing: {child.name}\\\")\\n        count = import_folder(store, child, category, depth=0)\\n        print(f\\\"       -> {count} nodes created\\\")\\n        total_nodes += count\\n\\n    stats = store.stats()\\n    print(f\\\"\\\\n=== Import Complete ===\\\")\\n    print(f\\\"Total nodes: {stats['total_nodes']}\\\")\\n    print(f\\\"Total links: {stats['total_links']}\\\")\\n    print(f\\\"Types: {stats['types']}\\\")\\n    print(f\\\"Owners: {stats['owners']}\\\")\\n\\n    return stats\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    import argparse\\n    parser = argparse.ArgumentParser(description=\\\"Import Hypernet Structure into graph store\\\")\\n    parser.add_argument(\\\"--source-dir\\\", default=\\\"C:/Hypernet/Hypernet Structure\\\",\\n                       help=\\\"Path to Hypernet Structure folder\\\")\\n    parser.add_argument(\\\"--data-dir\\\", default=\\\"C:/Hypernet/Hypernet Structure/0/0.1 - Hypernet Core/data\\\",\\n                       help=\\\"Path to store graph data\\\")\\n    args = parser.parse_args()\\n\\n    import_structure(args.source_dir, args.data_dir)\\n\"\n  },\n  \"created_at\": \"2026-02-15T11:54:04.213063+00:00\",\n  \"updated_at\": \"2026-02-15T11:54:04.213063+00:00\",\n  \"deleted_at\": null,\n  \"source_type\": \"import\",\n  \"source_id\": \"file:import_structure.py\"\n}"
  },
  "created_at": "2026-02-17T01:23:15.913248+00:00",
  "updated_at": "2026-02-17T01:23:15.913248+00:00",
  "deleted_at": null,
  "source_type": "import",
  "source_id": "file:node.json"
}