---
ha: "0.2.1"
object_type: "document"
creator: "1.1"
created: "2026-02-09"
status: "active"
visibility: "public"
flags: []
---

# 0.2.1 Storage Node Specification

## Purpose

Storage Nodes are the persistent memory of the Hypernet. They store encrypted Object data and participate in the global deduplication system that ensures every piece of information exists exactly once (with redundant copies for reliability).

## Core Functions

### 1. Object Storage
- Store encrypted Object chunks
- Maintain chunk metadata and checksums
- Respond to retrieval requests
- Verify data integrity

### 2. Deduplication Participation
- Content-addressable storage using cryptographic hashes
- Refuse storage of duplicate chunks
- Redirect duplicate requests to existing storage location

### 3. Replication
- Maintain required redundancy levels
- Participate in rebalancing operations
- Handle node failure recovery

### 4. Garbage Collection
- Track reference counts
- Remove orphaned data (with archival)
- Compact storage periodically

## Data Model

### Chunk Structure
```yaml
chunk:
  chunk_id: "[SHA-256 hash of content]"
  encrypted_content: "[AES-256-GCM encrypted data]"
  content_size: 12345  # bytes
  encryption_metadata:
    key_envelope: "[Encrypted key for authorized decryption]"
    nonce: "[Encryption nonce]"
  replication:
    target_copies: 3
    current_copies: 3
    locations: ["node_id_1", "node_id_2", "node_id_3"]
  metadata:
    created: "2025-01-30T12:00:00Z"
    last_verified: "2025-01-30T12:00:00Z"
    access_count: 42
    owner_object: "[Object address]"
```

### Chunk Size Limits
| Type | Minimum | Maximum | Typical |
|------|---------|---------|---------|
| Standard | 1 KB | 16 MB | 1 MB |
| Large Object | Uses chunking with merkle tree | | |

## Storage Operations

### PUT Operation
```
1. Receive encrypted chunk + metadata
2. Compute content hash
3. Check for existing chunk (dedup)
4. If duplicate: return existing location
5. If new: store and register with Cerberus
6. Initiate replication to peer nodes
7. Return success + chunk_id
```

### GET Operation
```
1. Receive chunk_id + access token
2. Verify access token with Cerberus
3. Retrieve chunk from storage
4. Verify integrity (checksum)
5. Return encrypted chunk
6. Log access for analytics
```

### DELETE Operation
```
1. Receive chunk_id + authorization
2. Verify delete permission with Cerberus
3. Decrement reference count
4. If references = 0: mark for archival
5. After archival period: remove from active storage
6. Maintain audit trail
```

## Redundancy Levels

| Data Type | Minimum Copies | Geographic Distribution |
|-----------|---------------|------------------------|
| System Critical | 7 | 3+ continents |
| Standard | 3 | 2+ regions |
| Archive | 2 | 1+ regions |
| Personal/Private | User-defined | User-defined |

## Performance Targets

| Metric | Target |
|--------|--------|
| Read latency (cached) | < 10 ms |
| Read latency (cold) | < 100 ms |
| Write latency | < 50 ms |
| Throughput per node | 1 Gbps |
| Availability | 99.99% |

## Economic Model

Storage nodes are compensated through:
- Storage fees (per GB per month)
- Retrieval fees (per request)
- Bandwidth fees (per GB transferred)

Compensation is distributed via the Hypernet token system (see 0.3.5).

## Related Documents

- 0.2.0 Node Architecture Overview
- 0.2.4 Node Registration Protocol
- 0.5.0 Master Object Schema
- 0.6.2 Object-to-Chunk Mapping
- 0.8.1 Encryption Standards
